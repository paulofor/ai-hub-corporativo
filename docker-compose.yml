services:
  backend:
    image: ${BACKEND_IMAGE:-ghcr.io/paulofor/ai-hub-corporativo-backend:latest}
    pull_policy: missing
    restart: unless-stopped
    command: >
      /bin/sh -c "if [ -f /run/secrets/openai-token/openai_api_key ]; then
        export OPENAI_API_KEY=$(cat /run/secrets/openai-token/openai_api_key);
      elif [ -f /run/secrets/openai-token/open_api_key ]; then
        export OPENAI_API_KEY=$(cat /run/secrets/openai-token/open_api_key);
      fi;
      exec java -jar /app/app.jar"
    env_file:
      - apps/backend/.env.example
      - .env
    depends_on:
      - sandbox-orchestrator
    volumes:
      - ./infra:/infra
      - ${OPENAI_TOKEN_HOST_DIR:-/root/infra/openai-token}:/run/secrets/openai-token:ro
    ports:
      - "${BACKEND_HTTP_PORT:-8081}:8081"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8081/actuator/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 120s
    networks:
      default:
        aliases:
          - backend
        ipv4_address: 172.28.0.10

  frontend:
    image: ${FRONTEND_IMAGE:-ghcr.io/paulofor/ai-hub-corporativo-frontend:latest}
    pull_policy: missing
    env_file:
      - apps/frontend/.env.example
      - .env
    depends_on:
      - backend
    volumes:
      - ./infra/downloads:/usr/share/nginx/html/downloads:ro
    ports:
      - "${FRONTEND_HTTP_PORT:-8082}:80"
    networks:
      default:
        ipv4_address: 172.28.0.11
      public-net:
        aliases:
          - aihub-frontend
        ipv4_address: 172.29.0.11

  reverse-proxy-cert-init:
    image: alpine:3.19
    restart: "no"
    command:
      - /bin/sh
      - -c
      - |
          set -euo pipefail
          apk add --no-cache openssl >/dev/null
          live_dir=/etc/letsencrypt/live/iahubcorp.online
          fullchain="$$live_dir/fullchain.pem"
          privkey="$$live_dir/privkey.pem"
          if [ -s "$$fullchain" ] && [ -s "$$privkey" ]; then
            echo "Existing certificate detected."
            exit 0
          fi
          mkdir -p "$$live_dir"
          openssl req -x509 -nodes -newkey rsa:2048 -days 365 \
            -keyout "$$privkey" \
            -out "$$fullchain" \
            -subj "/CN=iahubcorp.online"
          chmod 600 "$$privkey"
          chmod 644 "$$fullchain"
          echo "Self-signed certificate generated for iahubcorp.online."
    volumes:
      - ./infra/nginx/letsencrypt:/etc/letsencrypt

  reverse-proxy-conf-init:
    image: alpine:3.19
    restart: "no"
    command:
      - /bin/sh
      - -c
      - |
          set -euo pipefail
          mkdir -p /infra/nginx
          if [ -s /infra/nginx/frontend.conf ]; then
            echo "reverse-proxy-conf-init: frontend.conf j√° existe."
            exit 0
          fi
          cat <<'EOF' > /infra/nginx/frontend.conf
          map $http_upgrade $connection_upgrade {
            default upgrade;
            '' close;
          }

          # Permite uploads grandes (ex.: arquivos .zip enviados pelo /api/upload-jobs)
          # Valor alinhado ao limite de 500MB configurado no backend Spring Boot.
          client_max_body_size 500m;

          resolver 127.0.0.11 ipv6=off valid=30s;
          resolver_timeout 5s;

          upstream backend_upstream {
            zone backend_upstream 64k;
            server backend:8081;
          }

          upstream frontend_upstream {
            zone frontend_upstream 64k;
            server frontend:80;
          }

          server {
            listen 80;
            server_name iahubcorp.online www.iahubcorp.online;

            location /.well-known/acme-challenge/ {
              root /var/www/certbot;
            }

            location / {
              return 301 https://$host$request_uri;
            }
          }

          server {
            listen 443 ssl;
            server_name iahubcorp.online www.iahubcorp.online;

            ssl_certificate /etc/letsencrypt/live/iahubcorp.online/fullchain.pem;
            ssl_certificate_key /etc/letsencrypt/live/iahubcorp.online/privkey.pem;
            ssl_protocols TLSv1.2 TLSv1.3;
            ssl_prefer_server_ciphers on;
            ssl_session_cache shared:SSL:10m;

            add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;

            location /.well-known/acme-challenge/ {
              root /var/www/certbot;
            }

            location /api/ {
              proxy_pass http://backend_upstream;
              proxy_http_version 1.1;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
              proxy_set_header Upgrade $http_upgrade;
              proxy_set_header Connection $connection_upgrade;
            }

            location / {
              proxy_pass http://frontend_upstream;
              proxy_http_version 1.1;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
              proxy_set_header Upgrade $http_upgrade;
              proxy_set_header Connection $connection_upgrade;
            }
          }
          EOF
          echo "reverse-proxy-conf-init: frontend.conf criado em /infra/nginx/frontend.conf."
    volumes:
      - ./infra:/infra
    networks:
      default:
        ipv4_address: 172.28.0.12

  reverse-proxy:
    image: nginx:1.25-alpine
    restart: unless-stopped
    environment:
      NGINX_RELOAD_INTERVAL: 6h
    command:
      - /bin/sh
      - -c
      - |
          set -e
          nginx -g "daemon off;" &
          nginx_pid=$$!
          while :; do
            sleep "$${NGINX_RELOAD_INTERVAL}"
            nginx -s reload || true
          done &
          wait "$$nginx_pid"
    depends_on:
      reverse-proxy-conf-init:
        condition: service_completed_successfully
      reverse-proxy-cert-init:
        condition: service_completed_successfully
      frontend:
        condition: service_started
      backend:
        condition: service_healthy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./infra/nginx/frontend.conf:/etc/nginx/conf.d/default.conf:ro
      - ./infra/nginx/letsencrypt:/etc/letsencrypt:ro
      - ./infra/nginx/certbot-www:/var/www/certbot:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://backend:8081/actuator/health >/dev/null || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      default:
        ipv4_address: 172.28.0.13
      public-net:
        aliases:
          - iahubcorp.online
        ipv4_address: 172.29.0.13

  certbot:
    image: certbot/certbot:v2.10.0
    profiles:
      - certbot
    command: ["--version"]
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./infra/nginx/letsencrypt:/etc/letsencrypt
      - ./infra/nginx/certbot-www:/var/www/certbot
    networks:
      default:
        ipv4_address: 172.28.0.14

  sandbox-orchestrator:
    image: ${SANDBOX_ORCHESTRATOR_IMAGE:-ghcr.io/paulofor/ai-hub-corporativo-sandbox:latest}
    pull_policy: missing
    env_file:
      - apps/sandbox-orchestrator/.env.example
      - .env
    volumes:
      - ${OPENAI_TOKEN_HOST_DIR:-/root/infra/openai-token}:/run/secrets/openai-token:ro
      - ${SANDBOX_WORKDIR:-/root/ai-hub-corporativo/src}:${SANDBOX_WORKDIR:-/root/ai-hub-corporativo/src}
    ports:
      - "${SANDBOX_ORCHESTRATOR_HTTP_PORT:-8083}:8080"
    command: >
      /bin/sh -c "if [ -f /run/secrets/openai-token/openai_api_key ]; then
        export OPENAI_API_KEY=$(cat /run/secrets/openai-token/openai_api_key);
      elif [ -f /run/secrets/openai-token/open_api_key ]; then
        export OPENAI_API_KEY=$(cat /run/secrets/openai-token/open_api_key);
      fi;
      exec node dist/src/index.js"
    networks:
      default:
        aliases:
          - sandbox-orchestrator
        ipv4_address: 172.28.0.15
      public-net:
        aliases:
          - sandbox-orchestrator
        ipv4_address: 172.29.0.15

networks:
  default:
    ipam:
      config:
        - subnet: 172.28.0.0/16
  public-net:
    ipam:
      config:
        - subnet: 172.29.0.0/16
